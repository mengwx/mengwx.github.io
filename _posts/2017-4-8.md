---
layout: post
title:  "Hybrid computing using a neural network with dynamic external memory 笔记" 
category: notes
author: "孟文霞"

---

**原文**：[[Hybrid computing using a neural network with dynamic external memory]](http://www.nature.com/nature/journal/v538/n7626/pdf/nature20101.pdf)     
**原文+笔记**：[[Hybrid computing using a neural network with dynamic external memory]](/images/blog/2017-4-8/1.pdf)   

**摘要**:Artificial neural networks are remarkably adept at sensory processing, sequence learning and reinforcement learning, but are limited in their ability to represent variables and data structures and to store data over long timescales, owing to the lack of an external memory. Here we introduce a machine learning model called a differentiable neural computer (DNC), which consists of a neural network that can read from and write to an external memory matrix, analogous to the random-access memory in a conventional computer. Like a conventional computer, it can use its memory to represent and manipulate complex data structures, but, like a neural network, it can learn to do so from data. When trained with supervised learning, we demonstrate that a DNC can successfully answer synthetic questions designed to emulate reasoning and inference problems in natural language. We show that it can learn tasks such as finding the shortest path between specified points and inferring the missing links in randomly generated graphs, and then generalize these tasks to specific graphs such as transport networks and family trees. When trained with reinforcement learning, a DNC can complete a moving blocks puzzle in which changing goals are specified by sequences of symbols. Taken together, our results demonstrate that DNCs have the capacity to solve complex, structured tasks that are inaccessible to neural networks without external read–write memory.       

Artificial neural networks 在一些领域已经得到了广泛的应用，但是在数据结构、基于时间序列的数据存储方面还受到external memory的限制。 differentiable neural computer (DNC)可以和传统计算机一样读写数据、处理数据，同时具有neural networks的性质来通过数据进行学习。通过监督学习，DNC可以通过自然语言来回答和解决问题，也可以解决一些例如最短路径的常见算法问题。通过加强学习，DNC可以解决moving blocks puzzle问题。总的来说，DNCs可以解决复杂的结构化的问题。    
## Introduction
**传统计算机存储方面的限制**：This is a major liability: as the memory demands of a task increase, these networks cannot allocate new storage dynamically, nor easily learn algorithms that act independently of the values realized by the task variables.   
**传统计算机数据表示方面的限制**：cognitive scientists and neuroscientists have argued that neural networks are limited in their ability to represent variables and data structures, and to store data over long timescales without interference.   

**DNC**：We aim to combine the advantages of neural and computational processing by providing a neural network with read–write access to external memory. The access is narrowly **focused**, **minimizing interference** among memoranda and **enabling long-term storage**. The whole system is differentiable, and can therefore be trained end-to-end with gradient descent, allowing the network to learn how to operate and organize the memory in a **goal-directed** manner.   
通过一个外部的可读写的memory来结合neural network和computational processing的优点(上文加黑)，这样的系统可以实现end-to-end的梯度下降，network自动学习如何组织和操作存储。   

## System overview
**Memory**：RAM.The behaviour of the network is independent of the memory size as long as the memory is not filled to capacity, which is why we view the memory as ‘external’.     
**Controller**:CPU.Controller operations are learned with gradient descent.      
**Conventional computers** use unique addresses to access memory contents,   
**a DNC** uses differentiable attention mechanisms to define distributions over the N rows, or ‘locations’, in the N × W memory matrix M. These distributions, which we call **weightings**(代表这个点被读、写操作的degree), represent the degree to which each location is involved in a read or write operation.
**Conventional computers** 使用单一线性地址。    
**a DNC** 有一个N × W的矩阵M，有W<sup>r</sup>和W<sup>w</sup>分别表示读和写，r是N行M[i,*]和W<sup>r</sup>的内积(并不改变M[i,j])。对于写有erase vector e 和 write vector v: M[i,j] ← M[i,j]\(1 − w<sup>w</sup>[i]e[j]) + w<sup>w</sup>[i]v[j].公式表示M[i,j]修改的同时，w<sup>w</sup>[i]减去erase vector e 再加上 write vector v(对M[i,j]进行了修改)。具体方法如下：    
![matrix M](/images/blog/2017-4-8/1.png)      

DNC分为四个部分：Controller、Read and write heads、Memory、Memory usage and temporal links。       

**Controller**：对外部数据的接收、传出处理后的结果。       
**Read and write heads**：一个write head和多个read head(图中是两个)，write head有两个vector，是a write and an erase vector，用来编辑N × W的矩阵M，并且write key 也可以找到之前编辑的地方继续编辑。read key 有三种模式(‘C’)、reading out locations either forwards (‘F’) 和 backwards (‘B’) in the order they were written.      
**Memory**：The N × W memory matrix M.      
**Memory usage and temporal links**：记录哪些vector正在被编辑和曾经编辑过。
Figure 1 如下图：    
![figure 1](/images/blog/2017-4-8/2.png)    

## Interaction between the heads and the memory
Mechanism 有三种不同的机制：Content lookup、temporal links 和 Allocation。   
**Content lookup**:通过controller发出的一个vector，来通过cos夹角比较和 memory 中的内容。通过这个cos的值来确定上文中的W<sup>r</sup>和W<sup>w</sup>值，注意的是，如果这个cos值较小，也可以访问 matrix M 中的数据，但是不能获得地址。因为 matrix M 的数据前后关联很大。key-value retrieval 可以快速找到需要的数据结构。    
**Temporal links matrix L**：是一个 N × N 的矩阵，如果写完j之后写j，L[i,j]趋近于1，反之趋于0。矩阵 L 与 vector w 的积 Lw 可以表示forwards，矩阵 L 的转置与vector w 的积 L<sup>⊤</sup>w可以表示backwards。这样 DNC 可以通过了解谁写了当前的数据来恢复之前的数据，甚至生成没有出现的数据。       
**Allocation**：每一个位置的‘usage’是一个0-1之间的数，并且一个没有使用的地址会被传到 write head ，usage在对该单元进行读和写的操作之后数值都会增大，这样可以帮助 controler 选择不经常使用的存储单元来进行释放。 allocation mechanism 是独立于 memory 的，所以可以使用更大的 memory 来替换先前的，并且不用重新进行训练。    
Attention Mechanisms 的设计主要是从计算的角度来考虑，Content lookup 使数据具有结构性，temporal links 可以使数据找到输入的序列，allocation 提供未使用的地址。     
**这一部分是模型的关键，content lookup部分还是有些没懂，需要再重新看第二遍。最后一段的总结也没看懂，mammalian hippocampus应该是一个术语，需要再查参考文献。**    


**我理解的DNC**：由Controller、read and write heads、Memory、Memory usage and temporal links四部分组成，controller控制输入和输出，read and write heads是很多的向量，write 部分包括 write vector 和 erase vector，read 部分有 read vector，这部分的读写通过Content lookup、temporal links 和 Allocation 三种机制来控制，从而实现数据结构的定位和权限限制、forward 和 backward (N × N matrix L)的操作和通过 usage 找到没有被使用的单元和不常用可以释放的单元，Memory 是一个 N × M 的矩阵，可以通过两个读写的公式来分别进行读写操作。    
由于Memory M 和其他的向量是分开存储的，所以扩充Memory M 的时候不需要重新进行训练。

## Synthetic question answering experiments
这是DNC系统的第一个应用，DNCs表现比long shortterm memory (LSTM) 和 neural Turing machine都好。其中需要注意的是，DNC处理question answering问题并没有进行预处理和句子特征方面的处理。

## 三个实例：Graph experiments、Synthetic question answering experiments、Block puzzle experiments
下面的三个DNCs的应用说明了其解决问题的能力和优越性，并在各方面都表现地由于LSTM。
以 Block puzzle experiments 为例，该问题要求把 Blocks 移动地符合要求的位置，举例，移动步骤如下图：    
![figure 19](/images/blog/2017-4-8/19.png)   
其中题目给出的要求如下图：    
![figure 20](/images/blog/2017-4-8/20.png)    
其中以最少的步数移动成功达到目标要求即为表现perfect，不是最少步为success，没有达到目标为incomplete，横坐标表示移动的 Block 数，图1为 DNCs 和 LSTM 相比两个分别的学习情况，其中 DNCs 可以训练成功，而LSTM不能。
![figure 20](/images/blog/2017-4-8/20.png)    

**bAbI**:对于上文的问答，可以把实体构造成node，并在node之间建立连接，例如“John is in the playground”可以构造为下图：    
![figure 3](/images/blog/2017-4-8/3.png)    

随机生成、并连接训练图，并定义了三种问答类型： ‘traversal’, ‘shortest path’ 和 ‘inference’ 
![figure 4](/images/blog/2017-4-8/4.png)    


## Discussion
DNCs可以处理图结构的数据，不论图中的节点是否明确给出。优异性和未来可用的方向。

## Methods    

## **1.Controller network**
### 相关向量 
* input vector：x<sup>t</sup> 
* output vector：y<sup>t</sup> 
* target vector：z<sup>t</sup>(supervised learning)
* action distribution(reinforcement learning)

Controller使用的是LSTM的网络结构，为了方便理解，先把LSTM的结构附图如下。(取自台大李宏毅老师的课件)
![figure 5](/images/blog/2017-4-8/5.png) 
### Step:
* controller 从 memory matrix M<sup>t-1</sup> 中读取向量R。
* 发出当前状态下的![figure 7](/images/blog/2017-4-8/7.png)向量(包含上文提到的读、写权限控制等信息，具体内容见下文)  
* 进行LSTM的迭代计算 计算过程如下图

![figure 8](/images/blog/2017-4-8/8.png)

其中从上到下的变量分别是：input gate, forget gate, state, output gate activation vectors, hidden.    
迭代过程图示如下(取自台大李宏毅老师的课件)：
![figure 6](/images/blog/2017-4-8/6.png) 
σ(x) = 1/(1 + exp(− x)) is the logistic sigmoid function, 初始值h<sup>0</sup>和s<sup>0</sup>都设为0。    
每一个time-setp，controller都会发出output vector和interface vector两个向量。 这两个向量分别表示当前time-step的输出和操作的性质(读写、权限、位置等)。  
![figure 9](/images/blog/2017-4-8/9.png)
假设 controller network 是recurrent的，可以把每一个 time-step 发出的两个向量封装起来。
![figure 10](/images/blog/2017-4-8/10.png)   

Interface parameters：展开后是如下的向量。    
![figure 11](/images/blog/2017-4-8/11.png)    
各个分享量的意义如下：    
![figure 12](/images/blog/2017-4-8/12.png)    
![figure 13](/images/blog/2017-4-8/13.png)    

## **2.Reading and writing to memory**
读或者写的具体地址取决于一个非负的向量。(这个向量所有元素之和小于等于1)
read vector：
![figure 14](/images/blog/2017-4-8/14.png)  
write vector：
![figure 15](/images/blog/2017-4-8/15.png)  
注意e为erase vector，与上文相同。

## **3.Memory addressing**
**where to write**：content-based addressing 、 dynamic memory allocation
**where to read**：content-based addressing 、 temporal memory linkage

Content-based addressing 操作的方式：     
![figure 16](/images/blog/2017-4-8/16.png)  
公式中k是lookup key，D 操作与求余弦值的方式类似。weghting C 在存储矩阵M中定义了正态分布，限制访问的隐节点的值被设置为0。

Dynamic memory allocation：     
![figure 17](/images/blog/2017-4-8/17.png)    
第一个公式表示有多少个 location 将要被 free gates 释放，第二个公式定义了 usage vector ，进行 write 操作后，usage vector 的值增大，最大为1。
a<sup>t</sup>是最新的可用的写地址：
![figure 18](/images/blog/2017-4-8/18.png)   

Write weighting 实现了controller的功能，用向量p<sup>t</sup>[i]表示哪一个i是最后被写的，L<sup>t</sup>[i,j]表示在 location j 写完之后又写了哪一个 location i，主要实现了下列几条功能：    
1.It can write to newly allocated locations.    
2.It locations addressed by content.    
3.It can choose not to write at all.   

## **4.Comparison with the neural Turing machine**
* First, the NTM has no mechanism to ensure that blocks of allocated memory do not overlap and interfere—a basic problem of computer memory management. Interference is not an issue for the dynamic memory allocation used by DNCs, which provides single free locations at a time, irrespective of index, and therefore does not require contiguous blocks. 
* Second, the NTM has no way of freeing locations that have already been written to and, hence, no way of reusing memory when processing long sequences. This problem is addressed in DNCs by the free gates used for de-allocation. 
* Third, sequential information is preserved only as long as the NTM continues to iterate through consecutive locations; as soon as the write head jumps to a different part of the memory (using content-based addressing) the order of writes before and after the jump cannot be recovered by the read head. The temporal link matrix used by DNCs does not suffer from this problem because it tracks the order in which writes were made.

与图灵机的区别，1.解决了 memory 中 overlap and interfere 的问题，2.对 freeing locations 有相应的记录的操作，3.读写跳跃可以恢复并回到上一个位置通过 tracks 。
