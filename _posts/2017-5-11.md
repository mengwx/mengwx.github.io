---
layout: post
title:  "概率图模型"
category: notes
author: "孟文霞"

---

### 有向图模型与无向图模型的对比
* 共同点：将复杂的联合分布分解为多因子的乘积
* 不同点：无向图模型的因子是势函数，需要全局归一；有向图模型的因子是概率分布，无须全局归一
* 优缺点：无向图模型中势函数的设计不受概率的约束，灵活性高，但全局归一化代价高，调节因子后无法看出对全局分布的影响；有向图无须全局归一，训练高效

## 贝叶斯网表示
* 贝叶斯网
* 朴素贝叶斯模型及贝叶斯网 
* 贝叶斯网的独立性
* 贝叶斯网的应用

### 朴素贝叶斯模型
朴素贝叶斯模型假设所有的事例属于若干两两互斥且包含所有事例情况的类(class)中的一个。模型还包括一定数量的可以观测到值的特征(feartures) X<sub>1</sub>,X<sub>2</sub>,…,X<sub>k</sub>，在给定事例的类的条件下，这些特征条件独立。因子分解：    
![1](/images/blog/2017-5-11/1.png)   

### 贝叶斯网
利用分布的条件独立性获得紧凑且自然的表示，但与朴素贝叶斯网络不同的是，不必限制其分布的表示必须满足强独立性假设。    
**网络结构**：用有向图表示，节点表示随机变量，边表示一个变量对另一个变量的直接影响。
![2](/images/blog/2017-5-11/2.png)   
注意到图中横向的概率值和都为1，表示在该横坐标的条件下，纵坐标事件发生的概率。

### 贝叶斯网的独立性
在上图中，给定父节点G时，L与网中的所有其他节点条件独立；给定父节点I时，S与网中的所有其他节点条件独立。    
贝叶斯网语义：X 的父节点给定时，X 和所有的非 X 的子节点都独立。![3](/images/blog/2017-5-11/3.png)     

**Local independencies(acyclic graph)**：
![4](/images/blog/2017-5-11/4.png) 

**I-MAP**: IMAP部分能大概理解意思，但是还不能明确叙述清楚。


## 无向图模型
* 马尔科夫网的提出
* 马尔科夫网
* 马尔科夫网与贝叶斯网的关系
* 马尔科夫网的应用

马尔科夫网的引入：左边的图给定 AC 时，BD 独立。中间的图 AC 是结果，结果已知的时候，原因不独立，即给定 AC 时，BD 不独立。图中两个结构完全相同，而方向不一样就会导致独立性出现问题。右边的图变量之间的影响是对称的，不强调方向，再考察独立性时，每个节点的影响是相同的，转化为无向图。
![5](/images/blog/2017-5-11/5.png) 
图中的交互影响是无向的，所以无法使用在给定其他节点时在一个节点上表示分布的标准CPD。我们需要更加对称的参数化方法。    
![6](/images/blog/2017-5-11/6.png)    
a, b只有两个状态，发生/不发生，假设 a<sub>0</sub> 为不发生，a<sub>1</sub> 为发生。后面的数字为事件相对应的函数值。左边的图可以观察出，a, b 的意见相同的概率较大。
![7](/images/blog/2017-5-11/7.png)  
Z 是所有可能的加和。上式P 可化为一个 0-1 的概率，想要求某件事情的可能时，可以把 a,b,c,d 代入，全部带入后可以得到 a,b,c,d 的分布。求某个变量的边缘分布时，可以把该变量取某个值时候，遍历其他所有变量得到的取值进行加和。     
![8](/images/blog/2017-5-11/8.jpg)    
**优点**：通过局部信息可以生成全局信息，改变局部的值时，只要修改打分函数即可，较为灵活。    
**缺点**：更改某一个打分函数时，很难理解这个改动对其他因果关系有什么影响，需要进行计算。      
分布的分解与独立性性质：
![9](/images/blog/2017-5-11/8.png)       
成功表述了贝叶斯网中未能准确表述的独立性性质，实际上也与分布P在图上的分离性性质相对应：给定A、C时，B、D分离，给定B、D时A、C分离。    

### 马尔科夫网的参数化
假定全连通图，所有变量都是 2 值的，则每条边上的每个因子有 4 个参数 (a<sub>0</sub>b<sub>0</sub>,a<sub>1</sub>b<sub>1</sub>,a<sub>0</sub>b<sub>1</sub>,a<sub>1</sub>b<sub>1</sub>)，图中参数总个数为 4C<sup>2</sup><sub>n</sub>，而要指明这个联合分布需要的参数总述是 2<sup>n</sup>-1。我们发现，这样的因子只能描述成对变量的交互影响，不能描述包含比较大的变量子集的值的组合的交互影响。如果我们知道哪些变量之间是独立的，这些参数将大大减少。    
![10](/images/blog/2017-5-11/10.png)     
按照上图的定义，a->X,b->Y,c->Z，可以得到如下结果，图中因子的取值即是对马尔科夫网络参数化的过程。    
![11](/images/blog/2017-5-11/11.png)    

### 吉布斯分布
D<sub>i</sub> 图中的节点，如a,b,c，吉布斯分布可以通过联合分布求出各变量的边缘分布(某个因子的所有状态的加和)，可以发现联合分布构造的边缘分布和之前直观构造的参数的影响因子不一致。如 a<sup>0</sup>b<sup>0</sup>的联合分布为0.13，而参数影响因子是30。造成不一致的原因是边缘分布的计算同时加入了其他因子的影响因素。即局部的影响放在整理结构中时发生了变化。可以通过局部分布构造整体结构，也可以通过整体结构计算出局部的概率。
![12](/images/blog/2017-5-11/12.png)     
将吉布斯分布参数化与图结构联系起来，如果参数化包含其辖域X与Y的一个因子，则实际上它们之间引入了一个直接的交互影响，对应的马尔科夫网结构包含一条连接X和Y的边。    

![13](/images/blog/2017-5-11/13.png)   
Complete subgraph:若一个图的每一对不同顶点恰有一条边相连，则称为完全图。完全图是每对顶点之间都恰连有一条边的简单图。(团：两两连接)     

### 因子约减
![14](/images/blog/2017-5-11/14.png)   
即看C=c<sub>1</sub>时和哪些变量相关，上文中进行因子约减之后在图中的表现为把影响因素C结点和其他结点的连接全部删除后得到的新图。

### 马尔科夫网的独立性
![15](/images/blog/2017-5-11/15.png)    
X<sub>1</sub>,X<sub>2</sub> ...X<sub>k</sub> 是一个路径，Z 是可观测的集合，假设X<sub>i</sub>在这个可观察的集 Z 中，则可以定义分离。    
![16](/images/blog/2017-5-11/16.png)    
分离的概念：假设 Z 分离了 X 和 Y，那么在 Z 给定之后，找不到任何从 X 到 Y 的路径。也就是给定 Z，X 与 Y 独立。I(H) 是所有 Z 分离的路径的变量的集合，也就是 Z 已知后独立的变量的集合。    

![17](/images/blog/2017-5-11/17.png)    
可靠性：给定一个分布 P，H 是对应的图结构(可能不唯一)。下面两者可以互推。    
如果 P 是吉布斯分部<-->则这个分布可以 I-Map.    
![18](/images/blog/2017-5-11/18.png)    
完备性：X ,Y 关于 Z 不分离，即给定 Z, X 和 Y 不独立，在某个分布下，X 和 Y 之间有一个分布函数。    
局部独立性：定义 X 和 Y 是一对随机变量，在整个空间中去掉这两个变量之后独立，也就是说 X 和 Y 之间的路径不能在原来的图结构中，即局部独立。

### 从分布到图
* 基于成对的马尔科夫独立性
* 基于局部马尔科夫独立性    

![19](/images/blog/2017-5-11/19.png)  
如果 P 是一个正的分布，不包含 0，假定 H 是图结构，对所有的 X, Y，即转化为求马尔科夫随机网络中最小的团(全连接结构)。    

### 从贝叶斯网到马尔科夫网
![20](/images/blog/2017-5-11/20.png)    
通过添加方向，把马尔科夫网络转化为贝叶斯网络。求最小的I-Map，没有完备的贝叶斯网络在 H 中有完备的映射，即将两个图的I-map相同。(老师你在讲啥??? :()
![21](/images/blog/2017-5-11/21.png)  

## 马尔科夫条件随机场
条件随机场模型是Lafferty于2001年，在最大熵模型和隐马尔科夫模型的基础上，提出的一种判别式概率无向图学习模型，是一种用于标注和切分有序数据的条件概率模型。    

CRF最早是针对序列数据分析提出的，现已成功应用于自然语言处理 (Natural Language Processing，NLP） 、生物信息学、机器视觉及网络智能等领域。    

### 产生式模型和判别式模型
o和s分别表示观察序列和标记序列。
**产生式模型**：构建o和s的联合分布P(s,o)，可以根据联合概率来生成样本。如HMM,BNs,MRF。从统计的角度表示数据的分布情况，能够反映同类数据本身的相似度，不关心判别边界。    
无穷样本->概率密度模型=产生模型->预测
#### 优点
* 实际上带的信息要比判别模型丰富，研究单类问题比判别模型灵活性强
* 能更充分的利用先验知识
* 模型可以通过增量学习得到

#### 缺点
* 学习过程复杂
* 目标分类过程中容易产生很大的错误率
**判别式模型**：构建o和s的联合分布P(s|o)，因为没有s的知识，无法生成样本，只能判断分类，如SVM,CRF,MEMM。    
有限样本->判别函数()=预测模型->预测    

#### 两种模型的比较(例子)
![22](/images/blog/2017-5-11/22.png)  


**原文**：[[Belief Propagation in a 3D Spatio-temporal MRF for Moving Object Detection]](/images/blog/2017-4-20/1.pdf)     

**笔记**：[[Belief Propagation in a 3D Spatio-temporal MRF for Moving Object Detection 笔记]](/images/blog/2017-4-20/2.pdf)     

**注释**：笔记由LaTeX完成，内容见附件。    

### 最大熵模型(MEM)
* 其中 X 离散分布时是随机变量的个数
* 当 X 为确定值，即没有变化的可能时，左边等式成立
* 可以证明，当 X 服从均匀分布时，右边式子成立，均匀分布的熵最大

![23](/images/blog/2017-5-11/23.png)   
最大熵模型主要是在已有的一些限制条件下估计未知的概率分布。    

![24](/images/blog/2017-5-11/24.png)    
最大熵模型与马尔科夫模型类似都具有归一化因子，不同的是最大熵模型的归一化因子 Z 是指数加和。最大熵模型同样可以化成概率图。    

### 条件随机场(CRF)
随机场可以看成是一组随机变量的集合(这组随机变量对应同一个样本空间)。当给每一个位置按照某种分布随机赋予一个值后，其全体就叫做随机场。这些随机变量之间可能有依赖关系，一般来说，只有在变量间有依赖关系时，随机场才有十几意义。    

### 马尔科夫随机场(MRF)
对应一个无向图，这个无向图上的每一个节点对应一个随机变量，节点之间的边表示节点对应的随机变量之间有概率依赖关系。因此，MRF 的结构本质上反应了我们的先验知识——哪些变量之间有依赖关系需要考虑，而哪些可以忽略。    

**马尔科夫性质**：离当前因素比较遥远(这个遥远要根据具体情况自定义)的因素对当前因素的性质影响不大。    

如果给定的 MRF 中每个随机变量下面还有观察值，我们要确定的是给定观察集合下，这个 MRF 的分布，也就是条件分布，那么这个 MRF 就称为 CRF(条件随机场). 它的条件分布形式完全类似于 MRF 的分布形式，只不过多了一个观察集合 X.

## 概率图模型推理
* Brute force enumeration 
* Variable elimination algorithm
* Belief propagation algorithm

### Brute force enumeration   
通过暴力枚举对需要消去的变量求边缘分布：    
![25](/images/blog/2017-5-11/25.png)     
![26](/images/blog/2017-5-11/26.png)  

### Variable elimination algorithm    
* Push sums inside products (generalized distributive law)
* Carry out summations right to left, storing intermediate results (factors) to avoid recomputation (dynamic programming)     

(注：可以通过消去不相关的变量(置1)来减少计算)    
![27](/images/blog/2017-5-11/27.png)     
![28](/images/blog/2017-5-11/28.png)   


### Belief propagation algorithm
(Forwards‐backwards algorithm) 待整理