---
layout: post
title: "Random Forests"       
category: notes
author: "孟文霞"

---

**GitHub + 资料**：[[Random forest]](https://github.com/kjw0612/awesome-random-forest)      
**Scikit-Learn(Python)**：[[Scikit-Learn]](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble)     
**Lectures**：[[UBC Machine Learning]](http://www.cs.ubc.ca/~nando/540-2013/lectures.html)     

## Decision Trees
一个Decision Tree的例子：    
![1](/images/blog/2017-6-14/1.png)    
对于不同的数据会进入不同的节点中：
![2](/images/blog/2017-6-14/2.png)    
每一个节点都可以画出来一个包含不同种类数据的柱状统计图，在训练完成后，在柱状图中所占的比例可以判断为测试数据归属某一个类的概率：
![3](/images/blog/2017-6-14/3.png)    
节点应该选取那个特征来进行分类显得至关重要，我们希望最先选择可以把最多的不同种类数据分到不同分支下的特征，所以如何选择特征的方法是计算不同特征的 Information Gain,其中P<sub>i</sub>指 positive child的数量，n<sub>i</sub>值negative child的数量，计算方法如下：    
![4](/images/blog/2017-6-14/4.png)    

## Random Forests
Random Forests 的 Random 指两个部分的随机，一个是特征选取的随机，一个是数据点选取的随机。特征值选取的随机例子如下，选取的特征是 j=1(纵坐标) 和 j=3(横坐标)，然后构成二维坐标画出图像，分支决策点的选取是以节点在横(纵)坐标上的投影为准，同样选择计算出的最大的 Information Gain 的节点进行分割。
![5](/images/blog/2017-6-14/5.png)     
Random Forests 的 Random 也指对数据点选取时的随机，当数据量较大时，将所有节点都加入计算的代价过大，所以生成一颗树的数据往往是从数据池中抽样出来的。这样也同样可以解决数据某些特征缺失的问题，比传统的数据挖掘方法减少一项插入缺失值和处理极端值的工作。单独每一颗树受极端值的影响较大，而多颗树构成的 Forests 则受极端值的影响不大。    
### Random Forests Algorithm    
![6](/images/blog/2017-6-14/6.png)     
### Building a forest (ensemble)
![7](/images/blog/2017-6-14/7.png)     

### Random Forests 不同参数的 Effect
* Effect of forest size：Trees 的数量越多，不同种类的数据分界处会变得越平滑，数据倾向于以不同的概率缓慢向另一个种类过渡。
* Effect of tree depth (D)：depth 过深，可能会出现过拟合的现象，边界出现不规则的过度切割。    
* Effect of bagging：No bagging 时会出现 Max margin 现象，两个类的分界倾向于取边缘点的中点，但是黄色的边缘点距离整体数据距离较大，应当把界限向左移动，如下图。    
      
![8](/images/blog/2017-6-14/8.png) 