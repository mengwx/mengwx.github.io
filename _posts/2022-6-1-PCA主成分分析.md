---
layout: post
title:  "Principal Component Analysis, PCA"      
category: notes
author: "孟文霞"

---
@[TOC]

**理论解释**：[[特征值分解、奇异值分解、PCA概念整理]](http://blog.csdn.net/jinshengtao/article/details/18448355)     
[[机器学习中的数学 - 强大的矩阵奇异值分解(SVD)及其应用]](http://www.cnblogs.com/LeftNotEasy/archive/2011/01/19/svd-and-applications.html)      
[[从PCA和SVD的关系拾遗]](http://blog.csdn.net/dark_scope/article/details/53150883)   

## Algebraic Interpretation
Given m points in a n dimensional space, for large n, how does one project on to a low dimensional space while preserving broad trends in the data and allowing it to be visualized?

### Step 1. (Subtract the mean)
From each of the data dimensions. All the x values have mean subtracted and y values have mean subtracted from them. This produces a data set whose mean is zero. Subtracting the mean makes variance and covariance calculation easier by simplifying their equations. The variance and co-variance values are not affected by the mean value.

计算样例如下，x 的均值为 1.81，x<sub>1</sub> 变换后的值为 2.5-1.81=0.69，以此类推。    

```python
	>>> x = np.array([2.5,0.5,2.2,1.9,3.1,2.3,2.0,1.0,1.5,1.1])
	>>> y = np.array([2.4,0.7,2.9,2.2,3.0,2.7,1.6,1.1,1.6,0.9])
	>>> x1 = np.array([0.69,-1.31,0.39,0.09,1.29,0.49,0.19,-0.81,-0.31,-0.71])
	>>> y1 = np.array([0.49,-1.21,0.99,0.29,1.09,0.79,-0.31,-0.81,-0.31,-1.01])
```

### Step 2. (Calculate the covariance matrix)
协方差矩阵的计算可以用 numpy 包自带的计算工具，代码如下。        

```python
	>>> x1 = np.array([0.69,-1.31,0.39,0.09,1.29,0.49,0.19,-0.81,-0.31,-0.71])
	>>> y1 = np.array([0.49,-1.21,0.99,0.29,1.09,0.79,-0.31,-0.81,-0.31,-1.01])
	>>> np.cov(x1,y1)
	array([[ 0.61655556,  0.61544444],
	       [ 0.61544444,  0.71655556]])
```

Since the non-diagonal elements in this covariance matrix are positive, we should expect that both the x and y variable increase together.

### Step 3.(Calculate the eigenvectors and eigenvalues of the covariance matrix)
eigenvectors and eigenvalues 的计算可以用 numpy 包自带的计算工具，代码如下。     

```python
	>>> x = np.array([[ 0.61655556,  0.61544444],
	...  		   [ 0.61544444,  0.71655556]])
	>>> eigenvalues,eigenvectors = np.linalg.eig(x)
	>>> eigenvalues
	array([ 0.04908341,  1.28402771])
	>>> eigenvectors
	array([[-0.73517866, -0.6778734 ],
	       [ 0.6778734 , -0.73517866]])
```

### Step 4.(Reduce dimensionality and form feature vector)
* The eigenvector with the highest eigenvalue is the principle component of the data set.
* In our example, the eigenvector with the larges eigenvalue was the one that pointed down the middle of the data.
* Once eigenvectors are found from the covariance matrix, the next step is to order them by eigenvalue, highest to lowest.
* This gives you the components in order of significance.

Now, if you like, you can decide to ignore the components of lesser significance.
You do lose some information, but if the eigenvalues are small, you don’t lose much.

#### Operation
* N dimensions in your data 
* Calculate n eigenvectors and eigenvalues 
* Choose only the first p eigenvectors
* Final data set has only p dimensions.

#### Feature Vector
* FeatureVector = (eig<sub>1</sub> eig<sub>2</sub> eig<sub>3</sub> … eig<sub>n</sub>)
* We can either form a feature vector with both of the eigenvectors.
* Or, we can choose to leave out the smaller, less significant component and only have a single column.      

把 FeatureVector 中的 eig<sub>i</sub> 按照从小到大排序，选取最重要的，并改变对应的 eigenvectors 的值，形成新的 eigenvectors 矩阵。    
在 FeatureVector 对应的 eigenvectors 中，可以挑选比较重要的来选择，放弃掉对数据整体影响较小或者不重要的 eigenvectors.   

```python
	>>> eigenvalues = np.array([1.28402771, 0.04908341])
	>>> eigenvectors = np.array([[-0.6778734 , -0.73517866],
		[-0.73517866 , 0.6778734 ]])
```

### Step 5.(Deriving the new data)
FinalData = RowFeatureVector x RowZeroMeanData 

**RowFeatureVector** is the matrix with the eigenvectors in the columns transposed so that the eigenvectors are now in the rows, with the most significant eigenvector at the top. 

**RowZeroMeanData** is the mean-adjusted data transposed, ie. the data items are in each column, with each row holding a separate dimension.

FinalData transpose: dimensions along columns.

```python
	>>> eigenvectors = np.array([[-0.6778734,-0.73517866],
		[-0.73517866,0.6778734]])
	>>> xy = np.array([[0.69,-1.31,0.39,0.09,1.29,0.49,0.19,-0.81,-0.31,-0.71],
		[0.49,-1.21,0.99,0.29,1.09,0.79,-0.31,-0.81,-0.31,-1.01]])
	>>> eigenvectors.dot(xy)
	array([[-0.82797019,  1.77758033, -0.9921975 , -0.27421042, -1.67580143,
	        -0.91294911,  0.09910944,  1.14457217,  0.43804614,  1.22382056],
	       [-0.17511531,  0.14285723,  0.38437499,  0.13041721, -0.20949847,
	         0.17528244, -0.3498247 ,  0.04641726,  0.01776463, -0.16267529]])
```
